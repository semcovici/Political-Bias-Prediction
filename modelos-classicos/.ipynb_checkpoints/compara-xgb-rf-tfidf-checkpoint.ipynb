{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce62debc-bf37-4d18-85a9-dbd1069ebb20",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Importação das bibliotecas necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22df3ca1-3dbe-4119-a257-9e8412799283",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726dd049-2914-4e15-aa97-41f875bc5e6d",
   "metadata": {},
   "source": [
    "# Importação dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f094c27-9803-45b7-927f-418b6a45133e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../dataset/processed/artigos_de_partidos/artigos_partidos.csv')\n",
    "rem_cols = ['Partido', 'URL']\n",
    "data.drop(rem_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b506f40-5b6f-47c4-9ed9-5def04c2904b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Conteudo</th>\n",
       "      <th>Vies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Multa imposta ao candidato na condenação foi...</td>\n",
       "      <td>direita</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cadastro será usado como identificação junt...</td>\n",
       "      <td>direita</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A Bancada do NOVO na Câmara considera temerá...</td>\n",
       "      <td>direita</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Um ambiente com ausência de segurança juríd...</td>\n",
       "      <td>direita</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Segundo o MP, o estado do RJ sequer utiliza os...</td>\n",
       "      <td>direita</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Conteudo     Vies\n",
       "0  Multa imposta ao candidato na condenação foi...  direita\n",
       "1  Cadastro será usado como identificação junt...  direita\n",
       "2  A Bancada do NOVO na Câmara considera temerá...  direita\n",
       "3  Um ambiente com ausência de segurança juríd...  direita\n",
       "4  Segundo o MP, o estado do RJ sequer utiliza os...  direita"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head() # visualização das primeiras 5 linhas do dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2ef30cb-c985-4221-b0fb-48e617b6732c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# conversao dos rotulos categoricos para numericos\n",
    "data['Vies'] = data['Vies'].map({\n",
    "                                    'direita':2,\n",
    "                                    'centro': 1,\n",
    "                                    'esquerda': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4910bc1c-f52a-4bae-985f-a5282c1729a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Conteudo</th>\n",
       "      <th>Vies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Multa imposta ao candidato na condenação foi...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cadastro será usado como identificação junt...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A Bancada do NOVO na Câmara considera temerá...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Um ambiente com ausência de segurança juríd...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Segundo o MP, o estado do RJ sequer utiliza os...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Conteudo  Vies\n",
       "0  Multa imposta ao candidato na condenação foi...     2\n",
       "1  Cadastro será usado como identificação junt...     2\n",
       "2  A Bancada do NOVO na Câmara considera temerá...     2\n",
       "3  Um ambiente com ausência de segurança juríd...     2\n",
       "4  Segundo o MP, o estado do RJ sequer utiliza os...     2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defb5652-63fb-4fb7-b59f-16c9fbe8699a",
   "metadata": {},
   "source": [
    "# Divisão dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c659419-4e04-45cc-88c3-dbbeec61d993",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Conteudo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Multa imposta ao candidato na condenação foi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cadastro será usado como identificação junt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A Bancada do NOVO na Câmara considera temerá...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Um ambiente com ausência de segurança juríd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Segundo o MP, o estado do RJ sequer utiliza os...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Conteudo\n",
       "0  Multa imposta ao candidato na condenação foi...\n",
       "1  Cadastro será usado como identificação junt...\n",
       "2  A Bancada do NOVO na Câmara considera temerá...\n",
       "3  Um ambiente com ausência de segurança juríd...\n",
       "4  Segundo o MP, o estado do RJ sequer utiliza os..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a seguir os dados serão divididos entre features (X) e label (y)\n",
    "X_columns = [column for column in data.columns if column != 'Vies']\n",
    "X = data[X_columns]\n",
    "X.head() # features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56174d98-ac9b-4e9c-a9e1-784e917e8897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2\n",
       "1    2\n",
       "2    2\n",
       "3    2\n",
       "4    2\n",
       "Name: Vies, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data['Vies'] # label\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2843a037-0eae-458f-bca9-296fb683a646",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,\n",
    "                                                   stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a652599b-6b97-4591-a182-18e8b8519497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Conteudo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3285</th>\n",
       "      <td>Alagoas – O prefeito de Maceió, JHC (PL-AL), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5772</th>\n",
       "      <td>Após Indicação e Ofício apresentados pelo ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2306</th>\n",
       "      <td>Brasília – “Você importa. Escolha a vida!”. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1757</th>\n",
       "      <td>Nota do PCB Santa Catarina sobre a conquista d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690</th>\n",
       "      <td>O NOVO foi fundado em 2011 por pessoas comuns ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3568</th>\n",
       "      <td>Amazonas – A deputada estadual Therezinha Ruiz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4852</th>\n",
       "      <td>Brasília – O Ministério da Educação (MEC) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6461</th>\n",
       "      <td>O povo paraguaio está se insurgindo contra um...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10634</th>\n",
       "      <td>Na CCJ, governistas pedem vista e atrasam apre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10822</th>\n",
       "      <td>PV Pernambuco repudia as ofertas do governo fe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9370 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Conteudo\n",
       "3285   Alagoas – O prefeito de Maceió, JHC (PL-AL), ...\n",
       "5772   Após Indicação e Ofício apresentados pelo ...\n",
       "2306   Brasília – “Você importa. Escolha a vida!”. ...\n",
       "1757   Nota do PCB Santa Catarina sobre a conquista d...\n",
       "690    O NOVO foi fundado em 2011 por pessoas comuns ...\n",
       "...                                                  ...\n",
       "3568   Amazonas – A deputada estadual Therezinha Ruiz...\n",
       "4852   Brasília – O Ministério da Educação (MEC) ...\n",
       "6461   O povo paraguaio está se insurgindo contra um...\n",
       "10634  Na CCJ, governistas pedem vista e atrasam apre...\n",
       "10822  PV Pernambuco repudia as ofertas do governo fe...\n",
       "\n",
       "[9370 rows x 1 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "778008d7-b81e-434c-a3bd-858c69b36219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0\n",
       "0  1.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = TfidfVectorizer()\n",
    "vect = tf.fit_transform(X_train)\n",
    "pd.DataFrame.sparse.from_spmatrix(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324d4728-aaa3-4fed-8495-6a2feacba516",
   "metadata": {},
   "source": [
    "# Comparação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31550b8e-4684-401b-b4ac-e1cb5f04ce1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compara(iteracoes, modelos):\n",
    "    \n",
    "    # Escrita no arquivo\n",
    "    nome_arquivo = 'compara-xg-rf-tfidf.txt'\n",
    "    with open(nome_arquivo, \"w\") as arquivo:\n",
    "        pass\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    selection = SelectKBest()\n",
    "    \n",
    "    for model in modelos:\n",
    "    \n",
    "        param_grid = None\n",
    "        \n",
    "        scaler = MaxAbsScaler()\n",
    "\n",
    "        #preprocessor = ColumnTransformer(transformers=[\n",
    "        #                                ('vect_resp_text', vectorizer,'resp_text'), \n",
    "        #                                ('vect_pos', vectorizer,'pos')])\n",
    "\n",
    "        pipeline = Pipeline([\n",
    "                ('vect', vectorizer), \n",
    "                ('scaling', scaler), \n",
    "                ('selection', selection),\n",
    "                ('estimator', model)\n",
    "                ])\n",
    "\n",
    "        if isinstance(model, MultinomialNB):\n",
    "                param_grid = {\n",
    "                \"vect__ngram_range\": [(1,2), (1,3), (1,4), (2,3), (2,4), (3,4)],\n",
    "                \"vect__analyzer\": ['word','char'],\n",
    "                \"selection__k\": [200,400,600,800,1024],\n",
    "                \"estimator__alpha\": [50, 15, 10, 5, 1, 0.5, 0.3, 0.1, 0.05, 0.03, 0.02, 0.01,  0.001],\n",
    "                \"estimator__fit_prior\": [True, False],\n",
    "                }\n",
    "\n",
    "        if isinstance(model, SVC):\n",
    "                param_grid = {\n",
    "                \"vect__ngram_range\": [(1,2), (1,3), (1,4), (2,3), (2,4), (3,4)],\n",
    "                \"vect__analyzer\": ['word','char'],\n",
    "                \"selection__k\": [200,400,600,800,1024],\n",
    "                \"estimator__gamma\": [1, 0.1, 0.01, 0.001],\n",
    "                \"estimator__kernel\": ['linear', 'sigmoid'],\n",
    "                \"estimator__C\": [0.1, 1, 10, 100]\n",
    "                }\n",
    "\n",
    "\n",
    "        if isinstance(model, RandomForestClassifier):\n",
    "            param_grid = {\n",
    "            \"vect__ngram_range\": [(1,2), (1,3), (1,4), (2,3), (2,4), (3,4)],\n",
    "            \"vect__analyzer\": ['word','char'],\n",
    "            \"selection__k\": [200,400,600,800,1024],\n",
    "            \"estimator__n_estimators\": np.arange(20,150), \n",
    "            \"estimator__max_features\": ['log2', 'sqrt'],\n",
    "            \"estimator__max_depth\": np.arange(10,110),\n",
    "            \"estimator__min_samples_split\": np.arange(2,11),\n",
    "            \"estimator__min_samples_leaf\": np.arange(1,5),\n",
    "            \"estimator__bootstrap\": [True, False]\n",
    "            }\n",
    "            \n",
    "        if isinstance(model, XGBClassifier):\n",
    "            param_grid = {\n",
    "            \"vect__ngram_range\": [(1,2), (1,3), (1,4), (2,3), (2,4), (3,4)],\n",
    "            \"vect__analyzer\": ['word','char'],\n",
    "            \"selection__k\": [200,400,600,800,1024],\n",
    "            \"estimator__gamma\": np.linspace(0,9,100, dtype=np.int64),\n",
    "            \"estimator__alpha\": np.linspace(0,40,100, dtype=np.int64),\n",
    "            \"estimator__lambda\": np.linspace(0,3,10, dtype=np.int64),\n",
    "            \"estimator__colsample_bytree\": np.linspace(0.2,1,10, dtype=np.int64)\n",
    "            }\n",
    "\n",
    "        \n",
    "        # Prints do modelo e da vetorização\n",
    "        print(f'Modelo: {model}')\n",
    "            \n",
    "        print(f'Vetorizador utilizado: {vectorizer}')\n",
    "        \n",
    "        # Random Search\n",
    "        comeco_random_search = datetime.datetime.now()\n",
    "        print(f'Começo da Random Search: {comeco_random_search}')\n",
    "            \n",
    "        random_search = RandomizedSearchCV(pipeline, param_distributions=param_grid,cv=StratifiedKFold(n_splits=5),\n",
    "                                            n_iter=iteracoes, n_jobs=2, random_state=42)\n",
    "        \n",
    "        model_trained = random_search.fit(X_train, y_train)\n",
    "        \n",
    "        final_random_search = datetime.datetime.now()\n",
    "        print(f'Final da Random Search: {final_random_search}')\n",
    "        \n",
    "        score_random_search = model_trained.best_score_\n",
    "        score_random_search *= 100\n",
    "        score_random_search = round(score_random_search,2)\n",
    "        print(f'Melhor resultado na Random Search: {score_random_search}%')\n",
    "        \n",
    "        print('Melhores parâmetros encontrados:')\n",
    "        print(model_trained.best_params_)\n",
    "        \n",
    "        # Predição\n",
    "        y_pred = model_trained.predict(X_test)\n",
    "        acc_pred = accuracy_score(y_test, y_pred)\n",
    "        acc_pred *= 100\n",
    "        acc_pred = round(acc_pred,2)\n",
    "        print(f'Acurácia predita = {acc_pred}%')\n",
    "\n",
    "        report = classification_report(y_test, y_pred)\n",
    "        print(report)\n",
    "                \n",
    "        \n",
    "        print('----------------------------------------------')\n",
    "        \n",
    "        # Escrita no arquivo\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        with open(nome_arquivo, \"a\") as arquivo:\n",
    "            \n",
    "            arquivo.write(f'Modelo: {model}\\n')\n",
    "            arquivo.write(f'Vetorizador utilizado: {vectorizer}\\n')\n",
    "            arquivo.write(f'Começo da Random Search: {comeco_random_search}\\n')\n",
    "            arquivo.write(f'Final da Random Search: {final_random_search}\\n')\n",
    "            arquivo.write(f'Melhor resultado na Random Search: {score_random_search}\\n')\n",
    "            arquivo.write('Melhores parâmetros encontrados:\\n')\n",
    "            arquivo.write(str(model_trained.best_params_))\n",
    "            arquivo.write('\\n')\n",
    "            arquivo.write(f'Acurácia predita = {acc_pred}%\\n')\n",
    "            arquivo.write(f'Classification report: \\n')\n",
    "            arquivo.write(report)\n",
    "            arquivo.write('----------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88c646da-ded2-42f1-91b6-6ecf392bddbf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo: XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              predictor=None, random_state=None, ...)\n",
      "Vetorizador utilizado: TfidfVectorizer()\n",
      "Começo da Random Search: 2023-11-22 19:35:58.334838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Downloads2\\Anaconda\\download\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "5 fits failed out of a total of 5.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Downloads2\\Anaconda\\download\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Downloads2\\Anaconda\\download\\lib\\site-packages\\sklearn\\pipeline.py\", line 390, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"D:\\Downloads2\\Anaconda\\download\\lib\\site-packages\\sklearn\\pipeline.py\", line 348, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"D:\\Downloads2\\Anaconda\\download\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"D:\\Downloads2\\Anaconda\\download\\lib\\site-packages\\sklearn\\pipeline.py\", line 893, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"D:\\Downloads2\\Anaconda\\download\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 2077, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"D:\\Downloads2\\Anaconda\\download\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1330, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n",
      "  File \"D:\\Downloads2\\Anaconda\\download\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1220, in _count_vocab\n",
      "    raise ValueError(\n",
      "ValueError: empty vocabulary; perhaps the documents only contain stop words\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "D:\\Downloads2\\Anaconda\\download\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5868\\2471521786.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodelos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mXGBClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcompara\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodelos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5868\\999468116.py\u001b[0m in \u001b[0;36mcompara\u001b[1;34m(iteracoes, modelos)\u001b[0m\n\u001b[0;32m     84\u001b[0m                                             n_iter=iteracoes, n_jobs=2, random_state=42)\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m         \u001b[0mmodel_trained\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[0mfinal_random_search\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Downloads2\\Anaconda\\download\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    924\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    925\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 926\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    927\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Downloads2\\Anaconda\\download\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    388\u001b[0m         \"\"\"\n\u001b[0;32m    389\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 390\u001b[1;33m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    391\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Pipeline\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"passthrough\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Downloads2\\Anaconda\\download\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    346\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m             \u001b[1;31m# Fit or load from cache the current transformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 348\u001b[1;33m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[0;32m    349\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Downloads2\\Anaconda\\download\\lib\\site-packages\\joblib\\memory.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 349\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Downloads2\\Anaconda\\download\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    891\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    892\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"fit_transform\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 893\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    894\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    895\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Downloads2\\Anaconda\\download\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2075\u001b[0m         \"\"\"\n\u001b[0;32m   2076\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2077\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2078\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2079\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Downloads2\\Anaconda\\download\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1328\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1330\u001b[1;33m         \u001b[0mvocabulary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1332\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Downloads2\\Anaconda\\download\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1218\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1220\u001b[1;33m                 raise ValueError(\n\u001b[0m\u001b[0;32m   1221\u001b[0m                     \u001b[1;34m\"empty vocabulary; perhaps the documents only contain stop words\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1222\u001b[0m                 )\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "modelos = [XGBClassifier(seed=42),RandomForestClassifier(random_state=42)]\n",
    "compara(1, modelos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
