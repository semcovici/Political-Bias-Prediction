{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import model_selection, metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import string\n",
    "import textblob\n",
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-12 19:47:56.813154: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-12 19:47:56.896622: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-12 19:47:56.898735: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-12 19:47:59.673769: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import models, optimizers\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "from tensorflow.keras.layers import *\n",
    "# from tensorflow.keras.layers import LSTM, Convolution1D, GRU, Dense, Dropout, Input, Embedding, SpatialDropout1D, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for n in range(5):\n",
    "    df = pd.concat([\n",
    "        df,\n",
    "        pd.read_parquet(f'../../dataset/processed/artigos_de_partidos/artigos_partidos_nan.parquet')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = model_selection.train_test_split(df[\"Conteudo\"], df[\"Vies\"], stratify = df[\"Vies\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = preprocessing.LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_valid = encoder.fit_transform(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(analyzer = \"word\", token_pattern = r'\\w{1,}')\n",
    "count_vect.fit(df[\"Conteudo\"])\n",
    "X_train_count =  count_vect.transform(X_train)\n",
    "X_valid_count =  count_vect.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(analyzer = \"word\", max_features = 100)\n",
    "tfidf_vect.fit(df[\"Conteudo\"])\n",
    "X_train_tfidf = tfidf_vect.transform(X_train)\n",
    "X_valid_tfidf = tfidf_vect.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect_ngram = TfidfVectorizer(analyzer = \"word\", ngram_range = (1,3), max_features = 100)\n",
    "tfidf_vect_ngram.fit(df[\"Conteudo\"])\n",
    "X_train_tfidf_ngram =  tfidf_vect_ngram.transform(X_train)\n",
    "X_valid_tfidf_ngram =  tfidf_vect_ngram.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer = \"char\", ngram_range = (1,3), max_features = 100)\n",
    "tfidf_vect_ngram_chars.fit(df[\"Conteudo\"])\n",
    "X_train_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(X_train) \n",
    "X_valid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(X_valid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(df[\"Conteudo\"])\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seq = sequence.pad_sequences(token.texts_to_sequences(X_train), maxlen=150)\n",
    "X_valid_seq = sequence.pad_sequences(token.texts_to_sequences(X_valid), maxlen=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, X_valid, is_neural):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_valid)\n",
    "    if is_neural:\n",
    "        return metrics.accuracy_score(np.round(y_pred), y_valid)\n",
    "    else:\n",
    "        return metrics.accuracy_score(y_pred, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, Count Vectors:  0.9996585165960934\n",
      "LR, WordLevel TF-IDF:  0.8947548149159951\n",
      "LR, N-Gram Vectors:  0.8970086053817784\n",
      "LR, CharLevel Vectors:  0.7688840322360333\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(solver='liblinear')\n",
    "\n",
    "accuracy = train_model(\n",
    "    model, \n",
    "    X_train_count, \n",
    "    y_train, \n",
    "    X_valid_count, \n",
    "    False\n",
    ")\n",
    "print (\"LR, Count Vectors: \", accuracy)\n",
    "\n",
    "accuracy = train_model(\n",
    "    model, \n",
    "    X_train_tfidf, \n",
    "    y_train, \n",
    "    X_valid_tfidf, \n",
    "    False\n",
    ")\n",
    "print (\"LR, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "accuracy = train_model(\n",
    "    model, \n",
    "    X_train_tfidf_ngram, \n",
    "    y_train, \n",
    "    X_valid_tfidf_ngram, \n",
    "    False\n",
    ")\n",
    "print (\"LR, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "accuracy = train_model(\n",
    "    model, \n",
    "    X_train_tfidf_ngram_chars, \n",
    "    y_train, \n",
    "    X_valid_tfidf_ngram_chars, \n",
    "    False\n",
    ")\n",
    "print (\"LR, CharLevel Vectors: \", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
