{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7afa0e86-9e9c-4b5c-a67c-408e062dacf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == importando bibliotecas == \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "import textblob\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import model_selection, metrics\n",
    "from sklearn import preprocessing\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "from tensorflow.keras import models, optimizers\n",
    "# from tensorflow.keras.layers import LSTM, Convolution1D, GRU, Dense, Dropout, Input, Embedding, SpatialDropout1D, Bidirectional\n",
    "from tensorflow.keras.layers import *\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98642b37-b521-4bbd-a54e-8a5cd55f5f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # == download do modelo pré-treinado de word embedding == \n",
    "\n",
    "# inglês\n",
    "# !wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
    "# !gunzip cc.en.300.vec.gz\n",
    "# !mv cc.en.300.vec ../../dataset/fasttext_word_embedding/en_word_embedding.vec\n",
    "\n",
    "# português\n",
    "# !wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.pt.300.vec.gz\n",
    "# !gunzip cc.pt.300.vec.gz\n",
    "# !mv cc.pt.300.vec ../../fasttext_word_embedding/pt_word_embedding.vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6511c92-2c3e-443a-9e64-18115782c1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == variáveis == \n",
    "\n",
    "path_pt = 'dados_treino_ingles'\n",
    "path_en = 'dados_treino_pt_google_trad'\n",
    "path = path_pt\n",
    "word_embedding_en = 'en_word_embedding.vec'\n",
    "word_embedding_pt = 'pt_word_embedding.vec'\n",
    "word_embedding = word_embedding_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dfbfacf-ecd2-413b-b6df-9119c4a103e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == importar dados ==\n",
    "\n",
    "df = pd.read_parquet(f'../../dataset/processed/artigos_de_partidos/artigos_partidos.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b870bb3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PL              0.620759\n",
       "Novo            0.219512\n",
       "PP              0.107647\n",
       "União Brasil    0.052081\n",
       "Name: Partido, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "PSTU     0.242835\n",
       "PCB      0.238962\n",
       "PCDoB    0.233153\n",
       "PT       0.231216\n",
       "Rede     0.048025\n",
       "PSOL     0.005809\n",
       "Name: Partido, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "PSB    0.479065\n",
       "PV     0.235998\n",
       "MDB    0.163404\n",
       "PDT    0.121533\n",
       "Name: Partido, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# == contagem de artigos por viés ==\n",
    "\n",
    "df[df.Vies == 'direita'].Partido.value_counts() / df[df.Vies == 'direita'].shape[0]\n",
    "df[df.Vies == 'esquerda'].Partido.value_counts() / df[df.Vies == 'esquerda'].shape[0]\n",
    "df[df.Vies == 'centro'].Partido.value_counts() / df[df.Vies == 'centro'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6aa903cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PL              0.288995\n",
       "Novo            0.102194\n",
       "PP              0.050115\n",
       "União Brasil    0.024247\n",
       "Name: Partido, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "PSTU     0.053530\n",
       "PCB      0.052677\n",
       "PCDoB    0.051396\n",
       "PT       0.050969\n",
       "Rede     0.010587\n",
       "PSOL     0.001281\n",
       "Name: Partido, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "PSB    0.150431\n",
       "PV     0.074106\n",
       "MDB    0.051311\n",
       "PDT    0.038163\n",
       "Name: Partido, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# == contagem de artigos por viés ==\n",
    "\n",
    "df[df.Vies == 'direita'].Partido.value_counts() / df.shape[0]\n",
    "df[df.Vies == 'esquerda'].Partido.value_counts() / df.shape[0]\n",
    "df[df.Vies == 'centro'].Partido.value_counts() / df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0218d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == separar aproximadamente 5% de cada viés para usar como validação ==\n",
    "\n",
    "train = df[~df.Partido.isin(['PP', 'PT', 'MDB'])]\n",
    "valid = df[df.Partido.isin(['PP', 'PT', 'MDB'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99db238b-80d4-44fe-95bd-5abb6ce9bb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == train & test split ==\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train.Conteudo, valid.Conteudo, train.Vies, valid.Vies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33a17662-a983-4085-9e1f-9611ea2ebedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == label encoding do rótulo == \n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_valid = encoder.fit_transform(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ba053405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['centro', 'direita', 'esquerda'], dtype=object)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array(['centro', 'direita', 'esquerda'], dtype=object)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# == label das classes == \n",
    "\n",
    "encoder.classes_\n",
    "encoder.inverse_transform([0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87844994-9b70-4479-9194-46917cd46ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(token_pattern='\\\\w{1,}')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# == count vectorizer ==\n",
    "\n",
    "count_vect = CountVectorizer(\n",
    "    analyzer='word', \n",
    "    token_pattern=r'\\w{1,}'\n",
    ")\n",
    "count_vect.fit(df.Conteudo)\n",
    "X_train_count =  count_vect.transform(X_train)\n",
    "X_valid_count =  count_vect.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "538a3f53-3518-4c9b-b80e-87aa67b081b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_features=100)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# == word level tf-idf ==\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(\n",
    "    analyzer='word', \n",
    "    max_features=100\n",
    ")\n",
    "tfidf_vect.fit(df.Conteudo)\n",
    "X_train_tfidf = tfidf_vect.transform(X_train)\n",
    "X_valid_tfidf = tfidf_vect.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "890afb6b-d197-41e7-ba17-da09e3221124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_features=100, ngram_range=(1, 3))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# == ngram level tf-idf ==\n",
    "\n",
    "tfidf_vect_ngram = TfidfVectorizer(\n",
    "    analyzer='word', \n",
    "    ngram_range=(1,3), \n",
    "    max_features=100\n",
    ")\n",
    "tfidf_vect_ngram.fit(df.Conteudo)\n",
    "X_train_tfidf_ngram =  tfidf_vect_ngram.transform(X_train)\n",
    "X_valid_tfidf_ngram =  tfidf_vect_ngram.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85ffbd93-fe1d-4d6c-8ecb-7165f5e58817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='char', max_features=100, ngram_range=(1, 3))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# == characters level tf-idf ==\n",
    "\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(\n",
    "    analyzer='char', \n",
    "    ngram_range=(1,3), \n",
    "    max_features=100\n",
    ")\n",
    "tfidf_vect_ngram_chars.fit(df.Conteudo)\n",
    "X_train_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(X_train) \n",
    "X_valid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(X_valid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83c62f25-f015-4fc0-aa91-9abfc098a594",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000001it [02:01, 16424.59it/s]\n"
     ]
    }
   ],
   "source": [
    "# == fazer load do vetor pré-treinado de word embedding ==  \n",
    "\n",
    "embedding_idx = {}\n",
    "for i, line in tqdm(enumerate(open(f'../../dataset/fasttext_word_embedding/{word_embedding}'))):\n",
    "    values = line.split()\n",
    "    embedding_idx[values[0]] = np.asarray(values[1:] , dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce0e4360-f44e-466e-bddb-465de3945cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == criar um tokenizador ==\n",
    "\n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(df.Conteudo)\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ea3b051-3507-406b-a8a3-de95062fa200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == converter texto para sequência de tokens e preenchê-los para ter o mesmo tamanho == \n",
    "\n",
    "X_train_seq = sequence.pad_sequences(token.texts_to_sequences(X_train), maxlen=150)\n",
    "X_valid_seq = sequence.pad_sequences(token.texts_to_sequences(X_valid), maxlen=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75b0f56b-ede8-41b4-8385-d1620b52fe10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == criar map de token-embedding ==\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embedding_idx.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "390da503-ac42-4e8a-89d7-e4442e66643c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == método para trieinar o modelo == \n",
    "\n",
    "def train_model(model, X_train, y_train, X_valid, is_neural):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_valid)\n",
    "    res = pd.DataFrame(\n",
    "        classification_report(\n",
    "            y_pred, \n",
    "            y_valid, \n",
    "            digits=2,\n",
    "            output_dict=True,\n",
    "            labels=encoder.inverse_transform([0,1,2])\n",
    "        )\n",
    "    ).T\n",
    "    res['support'] = res.support.apply(int)\n",
    "    if is_neural:\n",
    "        return res\n",
    "    else:\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bf6f439a-c944-4fbe-a132-9d2c8c0f31e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Encoders require their input to be uniformly strings or numbers. Got ['int', 'str']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/_encode.py\u001b[0m in \u001b[0;36m_unique_python\u001b[0;34m(values, return_inverse)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0muniques\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'str' and 'int'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-032309eb7f25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mX_valid_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"LR, Count Vectors: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-a71be4343526>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, X_train, y_train, X_valid, is_neural)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mdigits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0moutput_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         )\n\u001b[1;32m     14\u001b[0m     ).T\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   2146\u001b[0m         \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2147\u001b[0m         \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2148\u001b[0;31m         \u001b[0mzero_division\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mzero_division\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2149\u001b[0m     )\n\u001b[1;32m   2150\u001b[0m     \u001b[0mrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1551\u001b[0m         \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m         \u001b[0msamplewise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msamplewise\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m     )\n\u001b[1;32m   1555\u001b[0m     \u001b[0mtp_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMCM\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mmultilabel_confusion_matrix\u001b[0;34m(y_true, y_pred, sample_weight, labels, samplewise)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0mle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m         \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m         \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_label.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \"\"\"\n\u001b[1;32m     98\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/_encode.py\u001b[0m in \u001b[0;36m_unique\u001b[0;34m(values, return_inverse)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \"\"\"\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_unique_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_inverse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;31m# numerical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_inverse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/_encode.py\u001b[0m in \u001b[0;36m_unique_python\u001b[0;34m(values, return_inverse)\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mtypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         raise TypeError(\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0;34m\"Encoders require their input to be uniformly \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m             \u001b[0;34mf\"strings or numbers. Got {types}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         )\n",
      "\u001b[0;31mTypeError\u001b[0m: Encoders require their input to be uniformly strings or numbers. Got ['int', 'str']"
     ]
    }
   ],
   "source": [
    "# == logistic regression == \n",
    "\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "\n",
    "accuracy = train_model(\n",
    "    model, \n",
    "    X_train_count, \n",
    "    y_train, \n",
    "    X_valid_count, \n",
    "    False\n",
    ")\n",
    "print (\"LR, Count Vectors: \", accuracy)\n",
    "accuracy.style.background_gradient(\n",
    "        cmap='viridis',\n",
    "        subset=pd.IndexSlice['centro':'esquerda', :'f1-score']\n",
    "    )\n",
    "\n",
    "accuracy = train_model(\n",
    "    model, \n",
    "    X_train_tfidf, \n",
    "    y_train, \n",
    "    X_valid_tfidf, \n",
    "    False\n",
    ")\n",
    "print (\"LR, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "accuracy = train_model(\n",
    "    model, \n",
    "    X_train_tfidf_ngram, \n",
    "    y_train, \n",
    "    X_valid_tfidf_ngram, \n",
    "    False\n",
    ")\n",
    "print (\"LR, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "accuracy = train_model(\n",
    "    model, \n",
    "    X_train_tfidf_ngram_chars, \n",
    "    y_train, \n",
    "    X_valid_tfidf_ngram_chars, \n",
    "    False\n",
    ")\n",
    "print (\"LR, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "92517b6b-a7e9-435b-a2cc-305173ebb613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, N-Gram Vectors:  0.6364145658263305\n"
     ]
    }
   ],
   "source": [
    "# == support vector machine (svm)\n",
    "\n",
    "accuracy = train_model(\n",
    "    SVC(), \n",
    "    X_train_tfidf_ngram, \n",
    "    y_train, \n",
    "    X_valid_tfidf_ngram,\n",
    "    False\n",
    ")\n",
    "print (\"SVM, N-Gram Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fddfea1f-921f-4812-815e-d6a22710988f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, Count Vectors:  0.661624649859944\n",
      "RF, WordLevel TF-IDF:  0.6649859943977591\n"
     ]
    }
   ],
   "source": [
    "# == random forest classifier == \n",
    "\n",
    "accuracy = train_model(\n",
    "    RandomForestClassifier(\n",
    "        n_estimators=500\n",
    "    ), \n",
    "    X_train_count, \n",
    "    y_train, \n",
    "    X_valid_count,\n",
    "    False\n",
    ")\n",
    "print (\"RF, Count Vectors: \", accuracy)\n",
    "\n",
    "accuracy = train_model(\n",
    "    RandomForestClassifier(\n",
    "        n_estimators=500\n",
    "    ), \n",
    "    X_train_tfidf, \n",
    "    y_train, \n",
    "    X_valid_tfidf,\n",
    "    False\n",
    ")\n",
    "print (\"RF, WordLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dc47b706-b956-4d0f-8696-88bafdfcd287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgb, Count Vectors:  0.5949579831932773\n",
      "Xgb, WordLevel TF-IDF:  0.6431372549019608\n",
      "Xgb, CharLevel Vectors:  0.6470588235294118\n"
     ]
    }
   ],
   "source": [
    "# == gradient boost == \n",
    "\n",
    "model = XGBClassifier(\n",
    "    use_label_encoder=False, \n",
    "    eval_metric='mlogloss',\n",
    "    learning_rate=0.01,\n",
    "    n_estimators=500\n",
    ")\n",
    "\n",
    "accuracy = train_model(\n",
    "    model, \n",
    "    X_train_count.tocsc(), \n",
    "    y_train, \n",
    "    X_valid_count.tocsc(), \n",
    "    False\n",
    ")\n",
    "print (\"Xgb, Count Vectors: \", accuracy)\n",
    "\n",
    "accuracy = train_model(\n",
    "    model, \n",
    "    X_train_tfidf.tocsc(), \n",
    "    y_train, \n",
    "    X_valid_tfidf.tocsc(), \n",
    "    False\n",
    ")\n",
    "print (\"Xgb, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "accuracy = train_model(\n",
    "    model, \n",
    "    X_train_tfidf_ngram_chars.tocsc(), \n",
    "    y_train, \n",
    "    X_valid_tfidf_ngram_chars.tocsc(), \n",
    "    False\n",
    ")\n",
    "print (\"Xgb, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b598fe8e-7e73-4acc-8024-514133946ee7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"lstm\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-990a2d184374>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m accuracy = train_model(\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mX_train_seq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-990a2d184374>\u001b[0m in \u001b[0;36mlstm\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# camada LSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/trackable/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                 raise ValueError(\n\u001b[0;32m--> 233\u001b[0;31m                     \u001b[0;34mf'Input {input_index} of layer \"{layer_name}\" '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m                     \u001b[0;34m\"is incompatible with the layer: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                     \u001b[0;34mf\"expected ndim={spec.ndim}, found ndim={ndim}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"lstm\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 128)"
     ]
    }
   ],
   "source": [
    "# == arquitetura lstm == \n",
    "\n",
    "def lstm():\n",
    "    # limpar a sessão\n",
    "    keras.backend.clear_session()\n",
    "    # iniciar o modelo \n",
    "    model = keras.Sequential()\n",
    "    # camada de entrada\n",
    "    model.add(Input((150, )))\n",
    "    # camada de word embedding\n",
    "    model.add(Embedding(\n",
    "        len(word_index) + 1, \n",
    "        300, \n",
    "        weights=[embedding_matrix], \n",
    "        trainable=True\n",
    "    ))\n",
    "    model.add(Convolution1D(\n",
    "        128, \n",
    "        3, \n",
    "        activation=\"relu\"\n",
    "    ))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    # model.add(SpatialDropout1D(0.2))\n",
    "    # camada LSTM\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dropout(0.2))\n",
    "    # camadas de saída\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    # compilar o modelo\n",
    "    model.compile(optimizer=optimizers.Adam(), metrics='accuracy', loss='binary_crossentropy')\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "accuracy = train_model(\n",
    "    lstm(), \n",
    "    X_train_seq, \n",
    "    y_train, \n",
    "    X_valid_seq, \n",
    "    True\n",
    ")\n",
    "print (\"RNN-LSTM, Word Embeddings\",  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "33a22d93-455d-4cb8-8c9d-99800dc44ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 150, 300)          46842600  \n",
      "                                                                 \n",
      " spatial_dropout1d (SpatialD  (None, 150, 300)         0         \n",
      " ropout1D)                                                       \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 150, 100)         105600    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 148, 100)          30100     \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 100)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 50)                5050      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 50)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 46,983,401\n",
      "Trainable params: 46,983,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "627/627 [==============================] - 231s 362ms/step - loss: 0.6629\n",
      "209/209 [==============================] - 9s 39ms/step\n",
      "CNN, Word Embeddings 0.6541510845175766\n"
     ]
    }
   ],
   "source": [
    "# == arquitetura recurrent convolutional neural network (RCNN) == \n",
    "\n",
    "def rcnn():\n",
    "    # limpar a sessão\n",
    "    keras.backend.clear_session()\n",
    "    # iniciar o modelo \n",
    "    model = keras.Sequential()\n",
    "    # camada de entrada \n",
    "    model.add(Input((150, )))\n",
    "    # camada de word embedding \n",
    "    model.add(Embedding(\n",
    "        len(word_index) + 1, \n",
    "        300, \n",
    "        weights=[embedding_matrix], \n",
    "        trainable=True\n",
    "    ))\n",
    "    model.add(SpatialDropout1D(0.3))\n",
    "    # camada recurrent \n",
    "    model.add(Bidirectional(\n",
    "        GRU(\n",
    "            50, \n",
    "            return_sequences=True\n",
    "        )\n",
    "    ))\n",
    "    # camada convolucional\n",
    "    model.add(Convolution1D(\n",
    "        100, \n",
    "        3, \n",
    "        activation=\"relu\"\n",
    "    ))\n",
    "    # camada pooling \n",
    "    model.add(GlobalMaxPool1D())\n",
    "    # camada denso \n",
    "    model.add(Dense(\n",
    "        50, \n",
    "        activation=\"relu\"\n",
    "    ))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(\n",
    "        1, \n",
    "        activation=\"sigmoid\"\n",
    "    ))\n",
    "    # model compile\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(), \n",
    "        loss='binary_crossentropy'\n",
    "    )\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "accuracy = train_model(\n",
    "    rcnn(), \n",
    "    X_train_seq, \n",
    "    y_train, \n",
    "    X_valid_seq, \n",
    "    True\n",
    ")\n",
    "print (\"CNN, Word Embeddings\",  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d80c715c-2e9d-42b5-bf42-7de321b9290f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2939ac02-b630-4905-a320-12c49858cdb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m108"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
