{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7afa0e86-9e9c-4b5c-a67c-408e062dacf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == importando bibliotecas == \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "import textblob\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import model_selection, metrics\n",
    "from sklearn import preprocessing\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "from tensorflow.keras import models, optimizers\n",
    "# from tensorflow.keras.layers import LSTM, Convolution1D, GRU, Dense, Dropout, Input, Embedding, SpatialDropout1D, Bidirectional\n",
    "from tensorflow.keras.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98642b37-b521-4bbd-a54e-8a5cd55f5f6f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-11-13 09:10:47--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.pt.300.vec.gz\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 2600:9000:20ac:7600:13:6e38:acc0:93a1, 2600:9000:20ac:9200:13:6e38:acc0:93a1, 2600:9000:20ac:d600:13:6e38:acc0:93a1, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|2600:9000:20ac:7600:13:6e38:acc0:93a1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1271093660 (1,2G) [binary/octet-stream]\n",
      "Saving to: ‘cc.pt.300.vec.gz’\n",
      "\n",
      "cc.pt.300.vec.gz    100%[===================>]   1,18G  16,8MB/s    in 85s     \n",
      "\n",
      "2023-11-13 09:12:14 (14,3 MB/s) - ‘cc.pt.300.vec.gz’ saved [1271093660/1271093660]\n",
      "\n",
      "mv: cannot move 'cc.pt.300.vec' to '../dataset/fasttext_word_embedding/pt_word_embedding.vec': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# # == download do modelo pré-treinado de word embedding == \n",
    "\n",
    "# inglês\n",
    "# !wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
    "# !gunzip cc.en.300.vec.gz\n",
    "# !mv cc.en.300.vec ../dataset/fasttext_word_embedding/en_word_embedding.vec\n",
    "\n",
    "# português\n",
    "# !wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.pt.300.vec.gz\n",
    "# !gunzip cc.pt.300.vec.gz\n",
    "# !mv cc.pt.300.vec ../dataset/fasttext_word_embedding/pt_word_embedding.vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6511c92-2c3e-443a-9e64-18115782c1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == variáveis == \n",
    "\n",
    "path_pt = 'dados_treino_ingles'\n",
    "path_en = 'dados_treino_pt_google_trad'\n",
    "path = path_en\n",
    "word_embedding_en = 'en_word_embedding.vec'\n",
    "word_embedding_pt = 'pt_word_embedding.vec'\n",
    "word_embedding = word_embedding_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dfbfacf-ecd2-413b-b6df-9119c4a103e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../dataset/dados_treino_pt_google_trad/parte_1.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-68d939f74bf2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     df = pd.concat([\n\u001b[1;32m      6\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'../dataset/{path}/parte_{n+1}.parquet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     ])\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, **kwargs)\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0muse_nullable_dtypes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_nullable_dtypes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m     )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, path, columns, use_nullable_dtypes, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"filesystem\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m             \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         )\n\u001b[1;32m    238\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;31m# this branch is used for example when reading from non-fsspec URLs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         handles = get_handle(\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mpath_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         )\n\u001b[1;32m    104\u001b[0m         \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../dataset/dados_treino_pt_google_trad/parte_1.parquet'"
     ]
    }
   ],
   "source": [
    "# == importar dados ==\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for n in range(5):\n",
    "    df = pd.concat([\n",
    "        df,\n",
    "        pd.read_parquet(f'../dataset/{path}/parte_{n+1}.parquet')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99db238b-80d4-44fe-95bd-5abb6ce9bb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == train & test split ==\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = model_selection.train_test_split(\n",
    "                                         df.conteudo, \n",
    "                                         df.rotulo\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "33a17662-a983-4085-9e1f-9611ea2ebedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == label encoding do rótulo == \n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_valid = encoder.fit_transform(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87844994-9b70-4479-9194-46917cd46ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == count vectorizer ==\n",
    "\n",
    "count_vect = CountVectorizer(\n",
    "    analyzer='word', \n",
    "    token_pattern=r'\\w{1,}'\n",
    ")\n",
    "count_vect.fit(df.conteudo)\n",
    "X_train_count =  count_vect.transform(X_train)\n",
    "X_valid_count =  count_vect.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "538a3f53-3518-4c9b-b80e-87aa67b081b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == word level tf-idf ==\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(\n",
    "    analyzer='word', \n",
    "    max_features=100\n",
    ")\n",
    "tfidf_vect.fit(df.conteudo)\n",
    "X_train_tfidf = tfidf_vect.transform(X_train)\n",
    "X_valid_tfidf = tfidf_vect.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "890afb6b-d197-41e7-ba17-da09e3221124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == ngram level tf-idf ==\n",
    "\n",
    "tfidf_vect_ngram = TfidfVectorizer(\n",
    "    analyzer='word', \n",
    "    ngram_range=(1,3), \n",
    "    max_features=100\n",
    ")\n",
    "tfidf_vect_ngram.fit(df.conteudo)\n",
    "X_train_tfidf_ngram =  tfidf_vect_ngram.transform(X_train)\n",
    "X_valid_tfidf_ngram =  tfidf_vect_ngram.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85ffbd93-fe1d-4d6c-8ecb-7165f5e58817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == characters level tf-idf ==\n",
    "\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(\n",
    "    analyzer='char', \n",
    "    ngram_range=(1,3), \n",
    "    max_features=100\n",
    ")\n",
    "tfidf_vect_ngram_chars.fit(df.conteudo)\n",
    "X_train_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(X_train) \n",
    "X_valid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(X_valid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83c62f25-f015-4fc0-aa91-9abfc098a594",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000001it [01:42, 19573.23it/s]\n"
     ]
    }
   ],
   "source": [
    "# == fazer load do vetor pré-treinado de word embedding ==  \n",
    "\n",
    "embedding_idx = {}\n",
    "for i, line in tqdm(enumerate(open(f'../dataset/fasttext_word_embedding/{word_embedding}.vec'))):\n",
    "    values = line.split()\n",
    "    embedding_idx[values[0]] = np.asarray(values[1:] , dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce0e4360-f44e-466e-bddb-465de3945cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == criar um tokenizador ==\n",
    "\n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(df.conteudo)\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ea3b051-3507-406b-a8a3-de95062fa200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == converter texto para sequência de tokens e preenchê-los para ter o mesmo tamanho == \n",
    "\n",
    "X_train_seq = sequence.pad_sequences(token.texts_to_sequences(X_train), maxlen=150)\n",
    "X_valid_seq = sequence.pad_sequences(token.texts_to_sequences(X_valid), maxlen=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "75b0f56b-ede8-41b4-8385-d1620b52fe10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == criar map de token-embedding ==\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embedding_idx.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "390da503-ac42-4e8a-89d7-e4442e66643c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == método para trieinar o modelo == \n",
    "\n",
    "def train_model(model, X_train, y_train, X_valid, is_neural):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_valid)\n",
    "    if is_neural:\n",
    "        return metrics.accuracy_score(np.round(y_pred), y_valid)\n",
    "    else:\n",
    "        return metrics.accuracy_score(y_pred, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf6f439a-c944-4fbe-a132-9d2c8c0f31e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, Count Vectors:  0.7712789827973074\n",
      "LR, WordLevel TF-IDF:  0.6448765893792072\n",
      "LR, N-Gram Vectors:  0.6457741211667913\n",
      "LR, CharLevel Vectors:  0.5949139865370232\n"
     ]
    }
   ],
   "source": [
    "# == logistic regression == \n",
    "\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "\n",
    "accuracy = train_model(\n",
    "    model, \n",
    "    X_train_count, \n",
    "    y_train, \n",
    "    X_valid_count, \n",
    "    False\n",
    ")\n",
    "print (\"LR, Count Vectors: \", accuracy)\n",
    "\n",
    "accuracy = train_model(+\n",
    "    model, \n",
    "    X_train_tfidf, \n",
    "    y_train, \n",
    "    X_valid_tfidf, \n",
    "    False\n",
    ")\n",
    "print (\"LR, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "accuracy = train_model(\n",
    "    model, \n",
    "    X_train_tfidf_ngram, \n",
    "    y_train, \n",
    "    X_valid_tfidf_ngram, \n",
    "    False\n",
    ")\n",
    "print (\"LR, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "accuracy = train_model(\n",
    "    model, \n",
    "    X_train_tfidf_ngram_chars, \n",
    "    y_train, \n",
    "    X_valid_tfidf_ngram_chars, \n",
    "    False\n",
    ")\n",
    "print (\"LR, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "92517b6b-a7e9-435b-a2cc-305173ebb613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, N-Gram Vectors:  0.6851159311892296\n"
     ]
    }
   ],
   "source": [
    "# == support vector machine (svm)\n",
    "\n",
    "accuracy = train_model(\n",
    "    SVC(), \n",
    "    X_train_tfidf_ngram, \n",
    "    y_train, \n",
    "    X_valid_tfidf_ngram,\n",
    "    False\n",
    ")\n",
    "print (\"SVM, N-Gram Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fddfea1f-921f-4812-815e-d6a22710988f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, Count Vectors:  0.7096484667165296\n",
      "RF, WordLevel TF-IDF:  0.6830216903515333\n"
     ]
    }
   ],
   "source": [
    "# == random forest classifier == \n",
    "\n",
    "accuracy = train_model(\n",
    "    RandomForestClassifier(\n",
    "        n_estimators=500\n",
    "    ), \n",
    "    X_train_count, \n",
    "    y_train, \n",
    "    X_valid_count,\n",
    "    False\n",
    ")\n",
    "print (\"RF, Count Vectors: \", accuracy)\n",
    "\n",
    "accuracy = train_model(\n",
    "    RandomForestClassifier(\n",
    "        n_estimators=500\n",
    "    ), \n",
    "    X_train_tfidf, \n",
    "    y_train, \n",
    "    X_valid_tfidf,\n",
    "    False\n",
    ")\n",
    "print (\"RF, WordLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dc47b706-b956-4d0f-8696-88bafdfcd287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgb, Count Vectors:  0.7842931937172775\n",
      "Xgb, WordLevel TF-IDF:  0.6900523560209424\n",
      "Xgb, CharLevel Vectors:  0.656095736724009\n"
     ]
    }
   ],
   "source": [
    "# == gradient boost == \n",
    "\n",
    "model = XGBClassifier(\n",
    "    use_label_encoder=False, \n",
    "    eval_metric='mlogloss',\n",
    "    learning_rate=0.01,\n",
    "    n_estimators=500\n",
    ")\n",
    "\n",
    "accuracy = train_model(\n",
    "    model, \n",
    "    X_train_count.tocsc(), \n",
    "    y_train, \n",
    "    X_valid_count.tocsc(), \n",
    "    False\n",
    ")\n",
    "print (\"Xgb, Count Vectors: \", accuracy)\n",
    "\n",
    "accuracy = train_model(\n",
    "    model, \n",
    "    X_train_tfidf.tocsc(), \n",
    "    y_train, \n",
    "    X_valid_tfidf.tocsc(), \n",
    "    False\n",
    ")\n",
    "print (\"Xgb, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "accuracy = train_model(\n",
    "    model, \n",
    "    X_train_tfidf_ngram_chars.tocsc(), \n",
    "    y_train, \n",
    "    X_valid_tfidf_ngram_chars.tocsc(), \n",
    "    False\n",
    ")\n",
    "print (\"Xgb, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b598fe8e-7e73-4acc-8024-514133946ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == arquitetura lstm == \n",
    "\n",
    "def lstm():\n",
    "    # limpar a sessão\n",
    "    keras.backend.clear_session()\n",
    "    # iniciar o modelo \n",
    "    model = keras.Sequential()\n",
    "    # camada de entrada\n",
    "    model.add(Input((150, )))\n",
    "    # camada de word embedding\n",
    "    model.add(Embedding(\n",
    "        len(word_index) + 1, \n",
    "        300, \n",
    "        weights=[embedding_matrix], \n",
    "        trainable=True\n",
    "    ))\n",
    "    model.add(Convolution1D(\n",
    "        128, \n",
    "        3, \n",
    "        activation=\"relu\"\n",
    "    ))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    # model.add(SpatialDropout1D(0.2))\n",
    "    # camada LSTM\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dropout(0.2))\n",
    "    # camadas de saída\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    # compilar o modelo\n",
    "    model.compile(optimizer=optimizers.Adam(), metrics='accuracy', loss='binary_crossentropy')\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "accuracy = train_model(\n",
    "    lstm(), \n",
    "    X_train_seq, \n",
    "    y_train, \n",
    "    X_valid_seq, \n",
    "    True\n",
    ")\n",
    "print (\"RNN-LSTM, Word Embeddings\",  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "33a22d93-455d-4cb8-8c9d-99800dc44ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 150, 300)          46842600  \n",
      "                                                                 \n",
      " spatial_dropout1d (SpatialD  (None, 150, 300)         0         \n",
      " ropout1D)                                                       \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 150, 100)         105600    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 148, 100)          30100     \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 100)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 50)                5050      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 50)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 46,983,401\n",
      "Trainable params: 46,983,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "627/627 [==============================] - 231s 362ms/step - loss: 0.6629\n",
      "209/209 [==============================] - 9s 39ms/step\n",
      "CNN, Word Embeddings 0.6541510845175766\n"
     ]
    }
   ],
   "source": [
    "# == arquitetura recurrent convolutional neural network (RCNN) == \n",
    "\n",
    "def rcnn():\n",
    "    # limpar a sessão\n",
    "    keras.backend.clear_session()\n",
    "    # iniciar o modelo \n",
    "    model = keras.Sequential()\n",
    "    # camada de entrada \n",
    "    model.add(Input((150, )))\n",
    "    # camada de word embedding \n",
    "    model.add(Embedding(\n",
    "        len(word_index) + 1, \n",
    "        300, \n",
    "        weights=[embedding_matrix], \n",
    "        trainable=True\n",
    "    ))\n",
    "    model.add(SpatialDropout1D(0.3))\n",
    "    # camada recurrent \n",
    "    model.add(Bidirectional(\n",
    "        GRU(\n",
    "            50, \n",
    "            return_sequences=True\n",
    "        )\n",
    "    ))\n",
    "    # camada convolucional\n",
    "    model.add(Convolution1D(\n",
    "        100, \n",
    "        3, \n",
    "        activation=\"relu\"\n",
    "    ))\n",
    "    # camada pooling \n",
    "    model.add(GlobalMaxPool1D())\n",
    "    # camada denso \n",
    "    model.add(Dense(\n",
    "        50, \n",
    "        activation=\"relu\"\n",
    "    ))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(\n",
    "        1, \n",
    "        activation=\"sigmoid\"\n",
    "    ))\n",
    "    # model compile\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(), \n",
    "        loss='binary_crossentropy'\n",
    "    )\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "accuracy = train_model(\n",
    "    rcnn(), \n",
    "    X_train_seq, \n",
    "    y_train, \n",
    "    X_valid_seq, \n",
    "    True\n",
    ")\n",
    "print (\"CNN, Word Embeddings\",  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d80c715c-2e9d-42b5-bf42-7de321b9290f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2939ac02-b630-4905-a320-12c49858cdb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m108"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
