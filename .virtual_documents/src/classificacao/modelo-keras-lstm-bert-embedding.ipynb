
# !pip install keras==2.3.1
# !pip install git+https://www.github.com/keras-team/keras-contrib.git

# !pip install patchify    #To install and import other mentioned libraries  in code
# !pip install segmentation_models

from tensorflow import keras
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.layers import Conv2D, Conv1D
from tensorflow.keras.layers import MaxPooling2D, MaxPooling1D, Reshape
from tensorflow.keras import backend as k
from keras.layers import BatchNormalization


import tensorflow as tf
from tensorflow import keras 
from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dense, Activation, Dropout
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from transformers import BertTokenizer, TFBertModel
import numpy as np
import os
import pandas as pd
from sklearn.model_selection import train_test_split

from tensorflow.compat.v1.keras.layers import CuDNNLSTM


os.environ["WANDB_API_KEY"] = "0" ## to silence warning


try:
    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
    tf.config.experimental_connect_to_cluster(tpu)
    tf.tpu.experimental.initialize_tpu_system(tpu)
    strategy = tf.distribute.experimental.TPUStrategy(tpu)
except ValueError:
    strategy = tf.distribute.get_strategy() # for CPU and single GPU
    print('Number of replicas:', strategy.num_replicas_in_sync)





# hyperparameters
max_length = 512
batch_size = 128


# Bert Tokenizer
model_multingual = "bert-base-multilingual-uncased"
model_bertimbau_lg = "neuralmind/bert-large-portuguese-cased"
model_bertimbau = "neuralmind/bert-base-portuguese-cased"

model_name = model_bertimbau_lg

tokenizer = BertTokenizer.from_pretrained(model_name)


data_raw = pd.read_parquet('../../dataset/processed/artigos_tratados/artigos_tratados.parquet')


# tira as linhas sem conteudo
data_raw = data_raw[data_raw['Conteudo'] != '']

X = data_raw.drop(['URL', 'Vies'], axis = 1)
y = data_raw['Vies']

y = y.map({
    'direita':2,
    'centro':1,
    'esquerda': 0})

y = pd.get_dummies(y).values

X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify=X['Partido'])
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.10, random_state=42, stratify=X_train_val['Partido'])


X_train = X_train.drop(['Partido'],axis=1).values

X_test = X_test.drop(['Partido'],axis=1).values

X_val = X_val.drop(['Partido'],axis=1).values

# transforma em uma lista
X_train = X_train.reshape(X_train.shape[0])
X_test = X_test.reshape(X_test.shape[0])
X_val = X_val.reshape(X_val.shape[0])


def bert_encode(data):
    tokens = tokenizer.batch_encode_plus(data, max_length=max_length, padding='max_length', truncation=True)
    
    return tf.constant(tokens['input_ids'])


train_encoded = bert_encode(X_train)


dev_encoded = bert_encode(X_val)


train_dataset = (
    tf.data.Dataset
    .from_tensor_slices((train_encoded, y_train))
    .shuffle(100)
    .batch(batch_size)
)

dev_dataset = (
    tf.data.Dataset
    .from_tensor_slices((dev_encoded, y_val))
    .shuffle(100)
    .batch(batch_size)
)








METRICS = [
      # keras.metrics.TruePositives(name='tp'),
      # keras.metrics.FalsePositives(name='fp'),
      # keras.metrics.TrueNegatives(name='tn'),
      # keras.metrics.FalseNegatives(name='fn'), 
      keras.metrics.Precision(name='precision'),
      keras.metrics.Recall(name='recall'),
      keras.metrics.AUC(name='auc'),
      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve
      keras.metrics.Accuracy()
      ]


import optuna
from keras.models import Sequential
import tensorflow as tf
from optuna.integration import TFKerasPruningCallback
from keras.callbacks import EarlyStopping, ReduceLROnPlateau

# Função objetivo para a otimização do Optuna
def objective(trial):
    keras.backend.clear_session()

    # Parâmetros otimizáveis
    lstm_units = trial.suggest_int('lstm_units', 64, 256)
    dense_units = trial.suggest_int('dense_units', 64, 256)
    conv_filters = trial.suggest_int('conv_filters', 32, 256)
    dropout = trial.suggest_float('dropout', 0.1, 0.9)
    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)
    num_lstm_layers = trial.suggest_int('num_lstm_layers', 1, 3)
    lstm_units_per_layer = trial.suggest_int('lstm_units_per_layer', 64, 256)
    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])

    model = Sequential()
    model.add(Reshape((512, 1), input_shape=(512,)))

    model.add(Conv1D(filters=conv_filters, kernel_size=3, activation='relu'))
    model.add(MaxPooling1D(pool_size=2))
    model.add(BatchNormalization())

    model.add(Dense(20, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)))
    model.add(Dropout(dropout))
    model.add(BatchNormalization())

    for _ in range(num_lstm_layers):
        model.add(tf.keras.layers.Bidirectional(CuDNNLSTM(lstm_units_per_layer, return_sequences=True, kernel_regularizer=tf.keras.regularizers.l2(0.01))))
        model.add(Dropout(dropout))
        model.add(BatchNormalization())

    model.add(tf.keras.layers.Bidirectional(CuDNNLSTM(lstm_units, kernel_regularizer=tf.keras.regularizers.l2(0.01))))
    model.add(Dropout(dropout))
    model.add(BatchNormalization())

    model.add(Dense(dense_units, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)))
    model.add(Dropout(dropout))
    model.add(BatchNormalization())

    model.add(Dense(output_shape, activation='softmax'))

    model.compile(loss='categorical_crossentropy',
                  optimizer=tf.keras.optimizers.Adam(learning_rate),
                  metrics=METRICS)

    # Treinamento do modelo
    history = model.fit(
        train_dataset,
        epochs=30,  # Número de épocas reduzido para economizar tempo
        validation_data=dev_dataset,
        batch_size=batch_size,
        shuffle=True,
        callbacks=[
            EarlyStopping(monitor='val_prc', patience=5),
            ReduceLROnPlateau(monitor='val_prc', patience=3, factor=0.1),
            TFKerasPruningCallback(trial, monitor='val_prc')
        ],
        verbose=0
    )

    # Retorne a métrica que você deseja otimizar (por exemplo, acurácia no conjunto de validação)
    return history.history['val_prc'][-1]

# Supondo que você tenha um conjunto de treinamento (train_dataset) e um conjunto de validação (dev_dataset)
output_shape = 3  # Substitua pelo valor correto

# Crie um estudo Optuna
study = optuna.create_study(direction='maximize')



study.optimize(objective, n_trials=None, timeout=21600)  # Ajuste o número de trials conforme necessário

# Obtenha os melhores parâmetros
best_params = study.best_params
print(f"Melhores Parâmetros: {best_params}")


study.best_params





def build_lstm_model(params, epochs):
    
    keras.backend.clear_session()

    # Parâmetros otimizáveis
    lstm_units = params.get('lstm_units')
    dense_units = params.get('dense_units')
    conv_filters = params.get('conv_filters')
    dropout = params.get('dropout')
    learning_rate = params.get('learning_rate')
    num_lstm_layers = params.get('num_lstm_layers')
    lstm_units_per_layer =params.get('lstm_units_per_layer')
    batch_size = params.get('batch_size')

    model = Sequential()
    model.add(Reshape((512, 1), input_shape=(512,)))

    model.add(Conv1D(filters=conv_filters, kernel_size=3, activation='relu'))
    model.add(MaxPooling1D(pool_size=2))
    model.add(BatchNormalization())

    model.add(Dense(20, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)))
    model.add(Dropout(dropout))
    model.add(BatchNormalization())

    for _ in range(num_lstm_layers):
        model.add(tf.keras.layers.Bidirectional(CuDNNLSTM(lstm_units_per_layer, return_sequences=True, kernel_regularizer=tf.keras.regularizers.l2(0.01))))
        model.add(Dropout(dropout))
        model.add(BatchNormalization())

    model.add(tf.keras.layers.Bidirectional(CuDNNLSTM(lstm_units, kernel_regularizer=tf.keras.regularizers.l2(0.01))))
    model.add(Dropout(dropout))
    model.add(BatchNormalization())

    model.add(Dense(dense_units, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)))
    model.add(Dropout(dropout))
    model.add(BatchNormalization())

    model.add(Dense(output_shape, activation='softmax'))

    model.compile(loss='categorical_crossentropy',
                  optimizer=tf.keras.optimizers.Adam(learning_rate),
                  metrics=METRICS)
    
    # Treinamento do modelo
    history = model.fit(
        train_dataset,
        epochs=epochs,  # Número de épocas reduzido para economizar tempo
        validation_data=dev_dataset,
        batch_size=batch_size,
        shuffle=True,
        callbacks=[
            EarlyStopping(monitor='val_prc', patience=5),
            ReduceLROnPlateau(monitor='val_prc', patience=3, factor=0.1)
        ],
        verbose=True
    )
    
    return history, model



best_params = {'lstm_units': 129,
 'dense_units': 234,
 'conv_filters': 159,
 'dropout': 0.1103597732759015,
 'learning_rate': 0.0002179456099780203,
 'num_lstm_layers': 1,
 'lstm_units_per_layer': 240,
 'batch_size': 32}
history, model = build_lstm_model(best_params, epochs = 300)


import matplotlib.pyplot as plt

def plot_graphs(history, string):
  plt.plot(history.history[string])
  plt.plot(history.history['val_'+string])
  plt.xlabel("Epochs")
  plt.ylabel(string)
  plt.legend([string, 'val_'+string])
  plt.show()
  
plot_graphs(history, "prc")
plot_graphs(history, "loss")


test_encoded = bert_encode(X_test)

test_dataset = (
    tf.data.Dataset
    .from_tensor_slices(test_encoded)
    .batch(batch_size)
)

y_pred_proba = model.predict(test_dataset, batch_size=batch_size)
y_pred = y_pred_proba.argmax(axis =1)


y_test = y_test.argmax(axis =1)


y_pred_proba


from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred))






