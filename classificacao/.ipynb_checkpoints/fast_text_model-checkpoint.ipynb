{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7afa0e86-9e9c-4b5c-a67c-408e062dacf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == importando bibliotecas == \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "import textblob\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import model_selection, metrics\n",
    "from sklearn import preprocessing\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "from tensorflow.keras import models, optimizers\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, Embedding, SpatialDropout1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98642b37-b521-4bbd-a54e-8a5cd55f5f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # == download do modelo pré-treinado de word embedding == \n",
    "\n",
    "# inglês\n",
    "# !wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
    "# !gunzip cc.en.300.vec.gz\n",
    "# !mv cc.en.300.vec ../dataset/fasttext_word_embedding/en_word_embedding.vec\n",
    "\n",
    "# português\n",
    "# !wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.pt.300.vec.gz\n",
    "# !gunzip cc.pt.300.vec.gz\n",
    "# !mv cc.pt.300.vec ../dataset/fasttext_word_embedding/pt_word_embedding.vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6511c92-2c3e-443a-9e64-18115782c1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == variáveis == \n",
    "\n",
    "path_pt = 'dados_treino_ingles'\n",
    "path_en = 'dados_treino_pt_google_trad'\n",
    "path = path_en\n",
    "word_embedding_en = 'en_word_embedding.vec'\n",
    "word_embedding_pt = 'pt_word_embedding.vec'\n",
    "word_embedding = word_embedding_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dfbfacf-ecd2-413b-b6df-9119c4a103e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == importar dados ==\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for n in range(5):\n",
    "    df = pd.concat([\n",
    "        df,\n",
    "        pd.read_parquet(f'../dataset/{path}/parte_{n+1}.parquet')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99db238b-80d4-44fe-95bd-5abb6ce9bb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == train & test split ==\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = model_selection.train_test_split(\n",
    "                                         df.conteudo, \n",
    "                                         df.rotulo\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33a17662-a983-4085-9e1f-9611ea2ebedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == label encoding do rótulo == \n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_valid = encoder.fit_transform(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87844994-9b70-4479-9194-46917cd46ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == count vectorizer ==\n",
    "\n",
    "count_vect = CountVectorizer(\n",
    "    analyzer='word', \n",
    "    token_pattern=r'\\w{1,}'\n",
    ")\n",
    "count_vect.fit(df.conteudo)\n",
    "X_train_count =  count_vect.transform(X_train)\n",
    "X_valid_count =  count_vect.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "538a3f53-3518-4c9b-b80e-87aa67b081b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == word level tf-idf ==\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(\n",
    "    analyzer='word', \n",
    "    max_features=100\n",
    ")\n",
    "tfidf_vect.fit(df.conteudo)\n",
    "X_train_tfidf = tfidf_vect.transform(X_train)\n",
    "X_valid_tfidf = tfidf_vect.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "890afb6b-d197-41e7-ba17-da09e3221124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == ngram level tf-idf ==\n",
    "\n",
    "tfidf_vect_ngram = TfidfVectorizer(\n",
    "    analyzer='word', \n",
    "    ngram_range=(1,3), \n",
    "    max_features=100\n",
    ")\n",
    "tfidf_vect_ngram.fit(df.conteudo)\n",
    "X_train_tfidf_ngram =  tfidf_vect_ngram.transform(X_train)\n",
    "X_valid_tfidf_ngram =  tfidf_vect_ngram.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85ffbd93-fe1d-4d6c-8ecb-7165f5e58817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == characters level tf-idf ==\n",
    "\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(\n",
    "    analyzer='char', \n",
    "    ngram_range=(1,3), \n",
    "    max_features=100\n",
    ")\n",
    "tfidf_vect_ngram_chars.fit(df.conteudo)\n",
    "X_train_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(X_train) \n",
    "X_valid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(X_valid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83c62f25-f015-4fc0-aa91-9abfc098a594",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000001it [01:42, 19573.23it/s]\n"
     ]
    }
   ],
   "source": [
    "# == fazer load do vetor pré-treinado de word embedding ==  \n",
    "\n",
    "embedding_idx = {}\n",
    "for i, line in tqdm(enumerate(open(f'../dataset/fasttext_word_embedding/{word_embedding}.vec'))):\n",
    "    values = line.split()\n",
    "    embedding_idx[values[0]] = np.asarray(values[1:] , dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce0e4360-f44e-466e-bddb-465de3945cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == criar um tokenizador ==\n",
    "\n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(df.conteudo)\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ea3b051-3507-406b-a8a3-de95062fa200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == converter texto para sequência de tokens e preenchê-los para ter o mesmo tamanho == \n",
    "\n",
    "X_train_seq = sequence.pad_sequences(token.texts_to_sequences(X_train), maxlen=150)\n",
    "X_valid_seq = sequence.pad_sequences(token.texts_to_sequences(X_valid), maxlen=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "75b0f56b-ede8-41b4-8385-d1620b52fe10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == criar map de token-embedding ==\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embedding_idx.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "390da503-ac42-4e8a-89d7-e4442e66643c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == método para trieinar o modelo == \n",
    "\n",
    "def train_model(model, X_train, y_train, X_valid, is_neural_net):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_valid)\n",
    "    if is_neural_net:\n",
    "        y_pred = y_pred.argmax(axis=-1)\n",
    "    return metrics.accuracy_score(y_pred, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf6f439a-c944-4fbe-a132-9d2c8c0f31e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, Count Vectors:  0.7712789827973074\n",
      "LR, WordLevel TF-IDF:  0.6448765893792072\n",
      "LR, N-Gram Vectors:  0.6457741211667913\n",
      "LR, CharLevel Vectors:  0.5949139865370232\n"
     ]
    }
   ],
   "source": [
    "# == logistic regression == \n",
    "\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "\n",
    "accuracy = train_model(\n",
    "    model, \n",
    "    X_train_count, \n",
    "    y_train, \n",
    "    X_valid_count, \n",
    "    False\n",
    ")\n",
    "print (\"LR, Count Vectors: \", accuracy)\n",
    "\n",
    "accuracy = train_model(\n",
    "    model, \n",
    "    X_train_tfidf, \n",
    "    y_train, \n",
    "    X_valid_tfidf, \n",
    "    False\n",
    ")\n",
    "print (\"LR, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "accuracy = train_model(\n",
    "    model, \n",
    "    X_train_tfidf_ngram, \n",
    "    y_train, \n",
    "    X_valid_tfidf_ngram, \n",
    "    False\n",
    ")\n",
    "print (\"LR, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "accuracy = train_model(\n",
    "    model, \n",
    "    X_train_tfidf_ngram_chars, \n",
    "    y_train, \n",
    "    X_valid_tfidf_ngram_chars, \n",
    "    False\n",
    ")\n",
    "print (\"LR, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc47b706-b956-4d0f-8696-88bafdfcd287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgb, Count Vectors:  0.7201196709050112\n",
      "Xgb, WordLevel TF-IDF:  0.6622288706058339\n",
      "Xgb, CharLevel Vectors:  0.6225878833208676\n"
     ]
    }
   ],
   "source": [
    "# == gradient boost == \n",
    "\n",
    "model = XGBClassifier(\n",
    "    use_label_encoder=False, \n",
    "    eval_metric='mlogloss',\n",
    "    learning_rate=0.01\n",
    ")\n",
    "\n",
    "accuracy = train_model(\n",
    "    model, \n",
    "    X_train_count.tocsc(), \n",
    "    y_train, \n",
    "    X_valid_count.tocsc(), \n",
    "    False\n",
    ")\n",
    "print (\"Xgb, Count Vectors: \", accuracy)\n",
    "\n",
    "accuracy = train_model(\n",
    "    model, \n",
    "    X_train_tfidf.tocsc(), \n",
    "    y_train, \n",
    "    X_valid_tfidf.tocsc(), \n",
    "    False\n",
    ")\n",
    "print (\"Xgb, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "accuracy = train_model(\n",
    "    model, \n",
    "    X_train_tfidf_ngram_chars.tocsc(), \n",
    "    y_train, \n",
    "    X_valid_tfidf_ngram_chars.tocsc(), \n",
    "    False\n",
    ")\n",
    "print (\"Xgb, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b598fe8e-7e73-4acc-8024-514133946ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "627/627 [==============================] - 183s 285ms/step - loss: 0.6851\n",
      "209/209 [==============================] - 25s 108ms/step\n",
      "RNN-LSTM, Word Embeddings 0.4807778608825729\n"
     ]
    }
   ],
   "source": [
    "# == arquitetura lstm == \n",
    "\n",
    "def lstm():\n",
    "    # limpar a sessão\n",
    "    keras.backend.clear_session()\n",
    "    # iniciar o modelo \n",
    "    model = keras.Sequential()\n",
    "    # camada de entrada\n",
    "    model.add(Input((150, )))\n",
    "    # camada de word embedding\n",
    "    model.add(Embedding(\n",
    "        len(word_index) + 1, \n",
    "        300, \n",
    "        weights=[embedding_matrix], \n",
    "        trainable=False\n",
    "    ))\n",
    "    # model.add(SpatialDropout1D(0.3))\n",
    "    # camada LSTM\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(128, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(128, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dropout(0.2))\n",
    "    # camadas de saída\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    # compilar o modelo\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    return model\n",
    "\n",
    "accuracy = train_model(lstm(), X_train_seq, y_train, X_valid_seq, True)\n",
    "print (\"RNN-LSTM, Word Embeddings\",  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33a22d93-455d-4cb8-8c9d-99800dc44ae5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d80c715c-2e9d-42b5-bf42-7de321b9290f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2939ac02-b630-4905-a320-12c49858cdb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m108"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
