{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7afa0e86-9e9c-4b5c-a67c-408e062dacf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-08 17:17:34.643063: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-08 17:17:41.072365: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-10-08 17:17:41.088303: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-10-08 17:17:41.088334: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# == importando bibliotecas == \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "import textblob\n",
    "from sklearn.liner_model import LogisticRegression\n",
    "from sklearn import model_selection, metrics\n",
    "from sklearn import preprocessing\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "from tensorflow.keras import layers, models, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98642b37-b521-4bbd-a54e-8a5cd55f5f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # == download do modelo pré-treinado de word embedding == \n",
    "\n",
    "# !wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
    "# !gunzip cc.en.300.vec.gz\n",
    "# !mv cc.en.300.vec ../dataset/fasttest_word_embedding/en_word_embedding.vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8dfbfacf-ecd2-413b-b6df-9119c4a103e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == importar dados ==\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for n in range(5):\n",
    "    df = pd.concat([\n",
    "        df,\n",
    "        pd.read_parquet(f'../dataset/dados_treino_ingles/parte_{n+1}.parquet')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "99db238b-80d4-44fe-95bd-5abb6ce9bb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == train & test split ==\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = model_selection.train_test_split(df.conteudo, df.rotulo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "33a17662-a983-4085-9e1f-9611ea2ebedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == label encoding do rótulo == \n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_valid = encoder.fit_transform(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "87844994-9b70-4479-9194-46917cd46ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == count vectorizer ==\n",
    "\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(df.conteudo)\n",
    "X_train_count =  count_vect.transform(X_train)\n",
    "X_valid_count =  count_vect.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "538a3f53-3518-4c9b-b80e-87aa67b081b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == word level tf-idf ==\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', max_features=5000)\n",
    "tfidf_vect.fit(df.conteudo)\n",
    "X_train_tfidf = tfidf_vect.transform(X_train)\n",
    "X_valid_tfidf = tfidf_vect.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "890afb6b-d197-41e7-ba17-da09e3221124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == ngram level tf-idf ==\n",
    "\n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram.fit(df.conteudo)\n",
    "X_train_tfidf_ngram =  tfidf_vect_ngram.transform(X_train)\n",
    "X_valid_tfidf_ngram =  tfidf_vect_ngram.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "85ffbd93-fe1d-4d6c-8ecb-7165f5e58817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == characters level tf-idf ==\n",
    "\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram_chars.fit(df.conteudo)\n",
    "X_train_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(X_train) \n",
    "X_valid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(X_valid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c62f25-f015-4fc0-aa91-9abfc098a594",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "254622it [00:17, 14528.16it/s]"
     ]
    }
   ],
   "source": [
    "# == fazer load do vetor pré-treinado de word embedding ==  \n",
    "\n",
    "embedding_idx = {}\n",
    "for i, line in tqdm(enumerate(open('../dataset/en_word_embedding.vec'))):\n",
    "    values = line.split()\n",
    "    embedding_idx[values[0]] = np.asarray(values[1:], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce0e4360-f44e-466e-bddb-465de3945cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == criar um tokenizador ==\n",
    "\n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(df.conteudo)\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5ea3b051-3507-406b-a8a3-de95062fa200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == converter texto para sequência de tokens e preenchê-los para ter o mesmo tamanho == \n",
    "\n",
    "X_train_seq = sequence.pad_sequences(token.texts_to_sequences(X_train), maxlen=150)\n",
    "X_valid_seq = sequence.pad_sequences(token.texts_to_sequences(X_valid), maxlen=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75b0f56b-ede8-41b4-8385-d1620b52fe10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == criar map de token-embedding ==\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embedding_idx.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "390da503-ac42-4e8a-89d7-e4442e66643c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == método para trieinar o modelo == \n",
    "\n",
    "def train_model(model, X_train, y_train, X_valid, is_neural_net):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_valid)\n",
    "    if is_neural_net:\n",
    "        y_pred = y_pred.argmax(axis=-1)\n",
    "    return metrics.accuracy_score(y_pred, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bf6f439a-c944-4fbe-a132-9d2c8c0f31e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, Count Vectors:  0.5117427075542259\n",
      "LR, WordLevel TF-IDF:  0.5053103964098729\n",
      "LR, N-Gram Vectors:  0.5136873597606582\n",
      "LR, CharLevel Vectors:  0.49857890800299176\n"
     ]
    }
   ],
   "source": [
    "# == logistic regression == \n",
    "\n",
    "model = linear_model.LogisticRegression(solver='liblinear')\n",
    "\n",
    "accuracy = train_model(model, X_train_count, y_train, X_valid_count, False)\n",
    "print (\"LR, Count Vectors: \", accuracy)\n",
    "\n",
    "accuracy = train_model(model, X_train_tfidf, y_train, X_valid_tfidf, False)\n",
    "print (\"LR, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "accuracy = train_model(model, X_train_tfidf_ngram, y_train, X_valid_tfidf_ngram, False)\n",
    "print (\"LR, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "accuracy = train_model(model, X_train_tfidf_ngram_chars, y_train, X_valid_tfidf_ngram_chars, False)\n",
    "print (\"LR, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc47b706-b956-4d0f-8696-88bafdfcd287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgb, Count Vectors:  0.5228122662677637\n",
      "Xgb, WordLevel TF-IDF:  0.5265519820493643\n"
     ]
    }
   ],
   "source": [
    "# == gradient boost == \n",
    "\n",
    "model = XGBClassifier(\n",
    "    use_label_encoder=False, \n",
    "    eval_metric='mlogloss',\n",
    "    learning_rate=0.01\n",
    ")\n",
    "\n",
    "accuracy = train_model(model, X_train_count.tocsc(), y_train, X_valid_count.tocsc(), False)\n",
    "print (\"Xgb, Count Vectors: \", accuracy)\n",
    "\n",
    "accuracy = train_model(model, X_train_tfidf.tocsc(), y_train, X_valid_tfidf.tocsc(), False)\n",
    "print (\"Xgb, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "accuracy = train_model(model, X_train_tfidf_ngram_chars.tocsc(), y_train, X_valid_tfidf_ngram_chars.tocsc(), False)\n",
    "print (\"Xgb, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b598fe8e-7e73-4acc-8024-514133946ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == arquitetura lstm == \n",
    "\n",
    "def lstm():\n",
    "    keras.backend.clear_session()\n",
    "    # camada de entrada\n",
    "    input_layer = layers.Input((150, ))\n",
    "    # camada de word embedding\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "    # camada LSTM\n",
    "    lstm_layer = layers.LSTM(128, activation='relu')(embedding_layer)\n",
    "    # camadas de saída\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "    # compilar o modelo\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    return model\n",
    "\n",
    "accuracy = train_model(lstm(), X_train_seq, y_train, X_valid_seq, True)\n",
    "print (\"RNN-LSTM, Word Embeddings\",  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3270aa8-d37c-406c-88f1-e7c160975cf5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33a22d93-455d-4cb8-8c9d-99800dc44ae5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d80c715c-2e9d-42b5-bf42-7de321b9290f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2939ac02-b630-4905-a320-12c49858cdb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m108"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
